{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dis\n",
    "from taxi_env import TaxiEnv\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchPolicy:\n",
    "    def __init__(self, state_size, action_size, lr = 0.001):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(state_size, action_size)\n",
    "        ).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr = lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.tensor(state, dtype = torch.float, device = device)\n",
    "        probs = nn.functional.softmax(self.policy(state), dim = 0)\n",
    "        m = dis.Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "\n",
    "    def update(self, state, action, reward):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        state = torch.tensor(state, dtype = torch.float, device = device)\n",
    "        action = torch.tensor(action, dtype = torch.long)\n",
    "        \n",
    "        loss = reward * self.criterion(self.policy(state).cpu(), action)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "def train(fuel_limit = 5000, episodes = 5000, lr = 0.001, gamma = 0.99):\n",
    "    env = TaxiEnv()\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    state_size = len(get_state(obs))\n",
    "    action_size = 6\n",
    "\n",
    "    policy_model = PyTorchPolicy(state_size, action_size, lr = lr)\n",
    "\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = get_state(obs)\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob = policy_model.get_action(state)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state = get_state(obs)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            # print(state)\n",
    "            # env.render(action, reward)\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "        loss = torch.stack(log_probs) * -returns\n",
    "        loss = loss.sum()\n",
    "\n",
    "        policy_model.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        policy_model.optimizer.step()\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode {episode + 1}/{episodes}, Average Reward: {np.mean(rewards_per_episode[-10:]):.3f}\")\n",
    "\n",
    "    env.close()\n",
    "    return policy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10/5000, Average Reward: -20508.410\n",
      "Episode 20/5000, Average Reward: -18643.680\n",
      "Episode 30/5000, Average Reward: -18656.380\n",
      "Episode 40/5000, Average Reward: -16385.570\n",
      "Episode 50/5000, Average Reward: -18670.360\n",
      "Episode 60/5000, Average Reward: -14053.640\n",
      "Episode 70/5000, Average Reward: -17023.800\n",
      "Episode 80/5000, Average Reward: -16798.500\n",
      "Episode 90/5000, Average Reward: -17615.500\n",
      "Episode 100/5000, Average Reward: -15798.260\n",
      "Episode 110/5000, Average Reward: -15181.650\n",
      "Episode 120/5000, Average Reward: -14723.760\n",
      "Episode 130/5000, Average Reward: -12493.560\n",
      "Episode 140/5000, Average Reward: -11968.830\n",
      "Episode 150/5000, Average Reward: -12269.720\n",
      "Episode 160/5000, Average Reward: -10343.200\n",
      "Episode 170/5000, Average Reward: -12783.950\n",
      "Episode 180/5000, Average Reward: -12122.350\n",
      "Episode 190/5000, Average Reward: -14030.000\n",
      "Episode 200/5000, Average Reward: -14750.700\n",
      "Episode 210/5000, Average Reward: -12591.000\n",
      "Episode 220/5000, Average Reward: -9616.280\n",
      "Episode 230/5000, Average Reward: -9931.700\n",
      "Episode 240/5000, Average Reward: -12350.000\n",
      "Episode 250/5000, Average Reward: -8406.160\n",
      "Episode 260/5000, Average Reward: -9643.010\n",
      "Episode 270/5000, Average Reward: -11671.000\n",
      "Episode 280/5000, Average Reward: -11964.500\n",
      "Episode 290/5000, Average Reward: -11283.500\n",
      "Episode 300/5000, Average Reward: -11182.180\n",
      "Episode 310/5000, Average Reward: -10637.470\n",
      "Episode 320/5000, Average Reward: -11210.500\n",
      "Episode 330/5000, Average Reward: -9314.300\n",
      "Episode 340/5000, Average Reward: -9672.390\n",
      "Episode 350/5000, Average Reward: -7081.920\n",
      "Episode 360/5000, Average Reward: -10160.000\n",
      "Episode 370/5000, Average Reward: -8311.500\n",
      "Episode 380/5000, Average Reward: -8708.690\n",
      "Episode 390/5000, Average Reward: -9455.500\n",
      "Episode 400/5000, Average Reward: -8582.540\n",
      "Episode 410/5000, Average Reward: -9049.980\n",
      "Episode 420/5000, Average Reward: -8479.410\n",
      "Episode 430/5000, Average Reward: -9962.000\n",
      "Episode 440/5000, Average Reward: -10277.500\n",
      "Episode 450/5000, Average Reward: -7170.500\n",
      "Episode 460/5000, Average Reward: -6222.900\n",
      "Episode 470/5000, Average Reward: -7218.270\n",
      "Episode 480/5000, Average Reward: -7230.130\n",
      "Episode 490/5000, Average Reward: -6626.500\n",
      "Episode 500/5000, Average Reward: -7022.000\n",
      "Episode 510/5000, Average Reward: -7670.500\n",
      "Episode 520/5000, Average Reward: -6316.500\n",
      "Episode 530/5000, Average Reward: -5246.620\n",
      "Episode 540/5000, Average Reward: -7606.500\n",
      "Episode 550/5000, Average Reward: -8740.500\n",
      "Episode 560/5000, Average Reward: -8184.880\n",
      "Episode 570/5000, Average Reward: -4986.630\n",
      "Episode 580/5000, Average Reward: -6416.500\n",
      "Episode 590/5000, Average Reward: -5605.080\n",
      "Episode 600/5000, Average Reward: -6150.000\n",
      "Episode 610/5000, Average Reward: -6590.000\n",
      "Episode 620/5000, Average Reward: -7237.360\n",
      "Episode 630/5000, Average Reward: -4659.510\n",
      "Episode 640/5000, Average Reward: -8323.960\n",
      "Episode 650/5000, Average Reward: -4395.500\n",
      "Episode 660/5000, Average Reward: -6275.000\n",
      "Episode 670/5000, Average Reward: -5311.000\n",
      "Episode 680/5000, Average Reward: -7805.500\n",
      "Episode 690/5000, Average Reward: -4032.510\n",
      "Episode 700/5000, Average Reward: -3769.080\n",
      "Episode 710/5000, Average Reward: -6605.000\n",
      "Episode 720/5000, Average Reward: -4349.830\n",
      "Episode 730/5000, Average Reward: -5526.000\n",
      "Episode 740/5000, Average Reward: -4568.500\n",
      "Episode 750/5000, Average Reward: -6182.000\n",
      "Episode 760/5000, Average Reward: -7501.500\n",
      "Episode 770/5000, Average Reward: -4952.620\n",
      "Episode 780/5000, Average Reward: -3499.200\n",
      "Episode 790/5000, Average Reward: -7084.550\n",
      "Episode 800/5000, Average Reward: -6055.550\n",
      "Episode 810/5000, Average Reward: -4101.000\n",
      "Episode 820/5000, Average Reward: -4891.320\n",
      "Episode 830/5000, Average Reward: -8198.000\n",
      "Episode 840/5000, Average Reward: -6121.000\n",
      "Episode 850/5000, Average Reward: -3855.150\n",
      "Episode 860/5000, Average Reward: -4420.500\n",
      "Episode 870/5000, Average Reward: -6070.500\n",
      "Episode 880/5000, Average Reward: -3526.530\n",
      "Episode 890/5000, Average Reward: -8508.000\n",
      "Episode 900/5000, Average Reward: -9762.500\n",
      "Episode 910/5000, Average Reward: -4395.500\n",
      "Episode 920/5000, Average Reward: -3734.170\n",
      "Episode 930/5000, Average Reward: -3005.500\n",
      "Episode 940/5000, Average Reward: -4304.000\n",
      "Episode 950/5000, Average Reward: -3723.500\n",
      "Episode 960/5000, Average Reward: -6289.000\n",
      "Episode 970/5000, Average Reward: -3203.890\n",
      "Episode 980/5000, Average Reward: -3753.660\n",
      "Episode 990/5000, Average Reward: -5605.000\n",
      "Episode 1000/5000, Average Reward: -3777.530\n",
      "Episode 1010/5000, Average Reward: -5448.200\n",
      "Episode 1020/5000, Average Reward: -4719.000\n",
      "Episode 1030/5000, Average Reward: -4201.000\n",
      "Episode 1040/5000, Average Reward: -7251.500\n",
      "Episode 1050/5000, Average Reward: -4177.500\n",
      "Episode 1060/5000, Average Reward: -3057.000\n",
      "Episode 1070/5000, Average Reward: -6172.500\n",
      "Episode 1080/5000, Average Reward: -4148.000\n",
      "Episode 1090/5000, Average Reward: -4528.000\n",
      "Episode 1100/5000, Average Reward: -2864.500\n",
      "Episode 1110/5000, Average Reward: -2377.000\n",
      "Episode 1120/5000, Average Reward: -3759.000\n",
      "Episode 1130/5000, Average Reward: -2542.500\n",
      "Episode 1140/5000, Average Reward: -3993.000\n",
      "Episode 1150/5000, Average Reward: -5174.000\n",
      "Episode 1160/5000, Average Reward: -3554.500\n",
      "Episode 1170/5000, Average Reward: -3410.500\n",
      "Episode 1180/5000, Average Reward: -2509.500\n",
      "Episode 1190/5000, Average Reward: -4879.500\n",
      "Episode 1200/5000, Average Reward: -2648.590\n",
      "Episode 1210/5000, Average Reward: -3272.000\n",
      "Episode 1220/5000, Average Reward: -4820.000\n",
      "Episode 1230/5000, Average Reward: -5451.000\n",
      "Episode 1240/5000, Average Reward: -2804.000\n",
      "Episode 1250/5000, Average Reward: -3587.570\n",
      "Episode 1260/5000, Average Reward: -5085.000\n",
      "Episode 1270/5000, Average Reward: -2798.000\n",
      "Episode 1280/5000, Average Reward: -3412.000\n",
      "Episode 1290/5000, Average Reward: -5513.500\n",
      "Episode 1300/5000, Average Reward: -2432.650\n",
      "Episode 1310/5000, Average Reward: -2838.000\n",
      "Episode 1320/5000, Average Reward: -3391.000\n",
      "Episode 1330/5000, Average Reward: -2161.000\n",
      "Episode 1340/5000, Average Reward: -5246.000\n",
      "Episode 1350/5000, Average Reward: -3106.500\n",
      "Episode 1360/5000, Average Reward: -2106.500\n",
      "Episode 1370/5000, Average Reward: -6551.500\n",
      "Episode 1380/5000, Average Reward: -2925.500\n",
      "Episode 1390/5000, Average Reward: -8192.500\n",
      "Episode 1400/5000, Average Reward: -2407.500\n",
      "Episode 1410/5000, Average Reward: -2235.500\n",
      "Episode 1420/5000, Average Reward: -3062.210\n",
      "Episode 1430/5000, Average Reward: -3912.000\n",
      "Episode 1440/5000, Average Reward: -3509.000\n",
      "Episode 1450/5000, Average Reward: -4728.000\n",
      "Episode 1460/5000, Average Reward: -4291.500\n",
      "Episode 1470/5000, Average Reward: -6039.500\n",
      "Episode 1480/5000, Average Reward: -4140.000\n",
      "Episode 1490/5000, Average Reward: -2055.500\n",
      "Episode 1500/5000, Average Reward: -4603.000\n",
      "Episode 1510/5000, Average Reward: -2247.000\n",
      "Episode 1520/5000, Average Reward: -2979.500\n",
      "Episode 1530/5000, Average Reward: -1766.500\n",
      "Episode 1540/5000, Average Reward: -2058.000\n",
      "Episode 1550/5000, Average Reward: -3889.000\n",
      "Episode 1560/5000, Average Reward: -3987.000\n",
      "Episode 1570/5000, Average Reward: -2218.500\n",
      "Episode 1580/5000, Average Reward: -1934.500\n",
      "Episode 1590/5000, Average Reward: -1870.000\n",
      "Episode 1600/5000, Average Reward: -2577.000\n",
      "Episode 1610/5000, Average Reward: -1808.000\n",
      "Episode 1620/5000, Average Reward: -2145.500\n",
      "Episode 1630/5000, Average Reward: -2160.500\n",
      "Episode 1640/5000, Average Reward: -2104.000\n",
      "Episode 1650/5000, Average Reward: -1958.000\n",
      "Episode 1660/5000, Average Reward: -2036.500\n",
      "Episode 1670/5000, Average Reward: -1882.000\n",
      "Episode 1680/5000, Average Reward: -3391.500\n",
      "Episode 1690/5000, Average Reward: -1908.500\n",
      "Episode 1700/5000, Average Reward: -2669.500\n",
      "Episode 1710/5000, Average Reward: -1347.000\n",
      "Episode 1720/5000, Average Reward: -2975.000\n",
      "Episode 1730/5000, Average Reward: -2568.500\n",
      "Episode 1740/5000, Average Reward: -1915.000\n",
      "Episode 1750/5000, Average Reward: -4353.500\n",
      "Episode 1760/5000, Average Reward: -3148.000\n",
      "Episode 1770/5000, Average Reward: -1840.000\n",
      "Episode 1780/5000, Average Reward: -2662.500\n",
      "Episode 1790/5000, Average Reward: -1799.750\n",
      "Episode 1800/5000, Average Reward: -4433.760\n",
      "Episode 1810/5000, Average Reward: -4045.000\n",
      "Episode 1820/5000, Average Reward: -3261.500\n",
      "Episode 1830/5000, Average Reward: -3642.000\n",
      "Episode 1840/5000, Average Reward: -1859.000\n",
      "Episode 1850/5000, Average Reward: -2548.000\n",
      "Episode 1860/5000, Average Reward: -1689.000\n",
      "Episode 1870/5000, Average Reward: -1559.000\n",
      "Episode 1880/5000, Average Reward: -3696.500\n",
      "Episode 1890/5000, Average Reward: -1789.000\n",
      "Episode 1900/5000, Average Reward: -2202.000\n",
      "Episode 1910/5000, Average Reward: -2534.000\n",
      "Episode 1920/5000, Average Reward: -1689.000\n",
      "Episode 1930/5000, Average Reward: -3883.940\n",
      "Episode 1940/5000, Average Reward: -2718.500\n",
      "Episode 1950/5000, Average Reward: -1460.500\n",
      "Episode 1960/5000, Average Reward: -2225.000\n",
      "Episode 1970/5000, Average Reward: -5111.500\n",
      "Episode 1980/5000, Average Reward: -1585.500\n",
      "Episode 1990/5000, Average Reward: -3095.000\n",
      "Episode 2000/5000, Average Reward: -3618.000\n",
      "Episode 2010/5000, Average Reward: -1597.500\n",
      "Episode 2020/5000, Average Reward: -2195.000\n",
      "Episode 2030/5000, Average Reward: -2188.500\n",
      "Episode 2040/5000, Average Reward: -1880.000\n",
      "Episode 2050/5000, Average Reward: -1403.500\n",
      "Episode 2060/5000, Average Reward: -2739.500\n",
      "Episode 2070/5000, Average Reward: -2813.000\n",
      "Episode 2080/5000, Average Reward: -1529.500\n",
      "Episode 2090/5000, Average Reward: -1805.500\n",
      "Episode 2100/5000, Average Reward: -2439.710\n",
      "Episode 2110/5000, Average Reward: -1470.000\n",
      "Episode 2120/5000, Average Reward: -1360.000\n",
      "Episode 2130/5000, Average Reward: -1836.000\n",
      "Episode 2140/5000, Average Reward: -6724.000\n",
      "Episode 2150/5000, Average Reward: -1396.500\n",
      "Episode 2160/5000, Average Reward: -2351.000\n",
      "Episode 2170/5000, Average Reward: -1627.000\n",
      "Episode 2180/5000, Average Reward: -1579.500\n",
      "Episode 2190/5000, Average Reward: -2922.500\n",
      "Episode 2200/5000, Average Reward: -1379.500\n",
      "Episode 2210/5000, Average Reward: -2244.500\n",
      "Episode 2220/5000, Average Reward: -1586.500\n",
      "Episode 2230/5000, Average Reward: -1060.000\n",
      "Episode 2240/5000, Average Reward: -2664.000\n",
      "Episode 2250/5000, Average Reward: -1991.000\n",
      "Episode 2260/5000, Average Reward: -1938.000\n",
      "Episode 2270/5000, Average Reward: -6100.000\n",
      "Episode 2280/5000, Average Reward: -1133.000\n",
      "Episode 2290/5000, Average Reward: -1535.000\n",
      "Episode 2300/5000, Average Reward: -2439.720\n",
      "Episode 2310/5000, Average Reward: -1513.500\n",
      "Episode 2320/5000, Average Reward: -2664.000\n",
      "Episode 2330/5000, Average Reward: -3317.000\n",
      "Episode 2340/5000, Average Reward: -2999.500\n",
      "Episode 2350/5000, Average Reward: -3703.000\n",
      "Episode 2360/5000, Average Reward: -2059.500\n",
      "Episode 2370/5000, Average Reward: -1303.500\n",
      "Episode 2380/5000, Average Reward: -1027.000\n",
      "Episode 2390/5000, Average Reward: -1202.000\n",
      "Episode 2400/5000, Average Reward: -1330.500\n",
      "Episode 2410/5000, Average Reward: -4574.000\n",
      "Episode 2420/5000, Average Reward: -4603.500\n",
      "Episode 2430/5000, Average Reward: -1697.000\n",
      "Episode 2440/5000, Average Reward: -4535.000\n",
      "Episode 2450/5000, Average Reward: -1716.000\n",
      "Episode 2460/5000, Average Reward: -1408.500\n",
      "Episode 2470/5000, Average Reward: -1717.500\n",
      "Episode 2480/5000, Average Reward: -2511.500\n",
      "Episode 2490/5000, Average Reward: -867.500\n",
      "Episode 2500/5000, Average Reward: -3475.610\n",
      "Episode 2510/5000, Average Reward: -3473.000\n",
      "Episode 2520/5000, Average Reward: -1319.000\n",
      "Episode 2530/5000, Average Reward: -1304.500\n",
      "Episode 2540/5000, Average Reward: -6106.000\n",
      "Episode 2550/5000, Average Reward: -1801.500\n",
      "Episode 2560/5000, Average Reward: -959.000\n",
      "Episode 2570/5000, Average Reward: -1548.000\n",
      "Episode 2580/5000, Average Reward: -3566.500\n",
      "Episode 2590/5000, Average Reward: -1299.500\n",
      "Episode 2600/5000, Average Reward: -4173.620\n",
      "Episode 2610/5000, Average Reward: -1816.000\n",
      "Episode 2620/5000, Average Reward: -1419.500\n",
      "Episode 2630/5000, Average Reward: -5932.000\n",
      "Episode 2640/5000, Average Reward: -1040.500\n",
      "Episode 2650/5000, Average Reward: -2055.500\n",
      "Episode 2660/5000, Average Reward: -2427.500\n",
      "Episode 2670/5000, Average Reward: -3169.500\n",
      "Episode 2680/5000, Average Reward: -2104.500\n",
      "Episode 2690/5000, Average Reward: -2508.000\n",
      "Episode 2700/5000, Average Reward: -1533.500\n",
      "Episode 2710/5000, Average Reward: -1953.500\n",
      "Episode 2720/5000, Average Reward: -1184.490\n",
      "Episode 2730/5000, Average Reward: -1182.000\n",
      "Episode 2740/5000, Average Reward: -1624.500\n",
      "Episode 2750/5000, Average Reward: -1067.000\n",
      "Episode 2760/5000, Average Reward: -1556.000\n",
      "Episode 2770/5000, Average Reward: -1475.500\n"
     ]
    }
   ],
   "source": [
    "policy_model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_model.policy.state_dict(), \"policy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
